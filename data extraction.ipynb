{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: F:\\ba865 project data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('F:\\\\ba865 project data')\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total AI images: 6300\n",
      "Total Human images: 6300\n",
      "Total images in combined dataset: 12600\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data = {'ai': [], 'human': []}\n",
    "sample_size = 700\n",
    "\n",
    "base_image_dir = \"dataforuse\"\n",
    "\n",
    "def process_images(ai_sd_dir, ai_ld_dir, human_dir, ai_label, human_label):\n",
    "    ai_image_paths = []\n",
    "\n",
    "    if os.path.exists(ai_sd_dir):\n",
    "        ai_image_paths.extend([os.path.join(ai_sd_dir, image_name) for image_name in os.listdir(ai_sd_dir)])\n",
    "    if os.path.exists(ai_ld_dir):\n",
    "        ai_image_paths.extend([os.path.join(ai_ld_dir, image_name) for image_name in os.listdir(ai_ld_dir)])\n",
    "    \n",
    "    if len(ai_image_paths) > sample_size:\n",
    "        ai_image_paths = random.sample(ai_image_paths, sample_size)\n",
    "    \n",
    "    for image_path in ai_image_paths:\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((256, 256))\n",
    "            image = np.array(image) / 255.0\n",
    "            data[ai_label].append((image, ai_label))\n",
    "        except IOError as e:\n",
    "            print(f\"Could not read AI image {image_path}: {e}\")\n",
    "    \n",
    "    # Process Human images\n",
    "    if os.path.exists(human_dir):\n",
    "        human_image_names = os.listdir(human_dir)\n",
    "        if len(human_image_names) > sample_size:\n",
    "            human_image_names = random.sample(human_image_names, sample_size)\n",
    "        \n",
    "        for image_name in human_image_names:\n",
    "            image_path = os.path.join(human_dir, image_name)\n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "                image = image.resize((256, 256))\n",
    "                image = np.array(image) / 255.0\n",
    "                data[human_label].append((image, human_label))\n",
    "            except IOError as e:\n",
    "                print(f\"Could not read Human image {image_path}: {e}\")\n",
    "\n",
    "genres = [\"art_nouveau\", \"baroque\", \"expressionism\", \"impressionism\", \"post_impressionism\",\n",
    "          \"realism\", \"renaissance\", \"romanticism\", \"ukiyo-e\"]\n",
    "\n",
    "for genre in genres:\n",
    "    ai_sd_dir = os.path.join(base_image_dir, f\"AI_SD_{genre}\")\n",
    "    ai_ld_dir = os.path.join(base_image_dir, f\"AI_LD_{genre}\")\n",
    "    human_dir = os.path.join(base_image_dir, f\"Human_{genre}\")\n",
    "    process_images(ai_sd_dir, ai_ld_dir, human_dir, 'ai', 'human')\n",
    "\n",
    "combined_data = data['ai'] + data['human']\n",
    "\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "images, labels = zip(*combined_data)\n",
    "\n",
    "print(f\"Total AI images: {len(data['ai'])}\")\n",
    "print(f\"Total Human images: {len(data['human'])}\")\n",
    "print(f\"Total images in combined dataset: {len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in f:\\python\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in f:\\python\\lib\\site-packages\\numpy-1.24.1-py3.11-win-amd64.egg (from h5py) (1.24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "ai_output_filename = 'F:\\\\ba865 project data\\\\ai_artwork_dataset.h5'\n",
    "human_output_filename = 'F:\\\\ba865 project data\\\\human_artwork_dataset.h5'\n",
    "\n",
    "def save_images_to_hdf5(image_label_pairs, output_filename):\n",
    "    with h5py.File(output_filename, 'w') as hdf:\n",
    "        num_images = len(image_label_pairs)\n",
    "        \n",
    "        images_dset = hdf.create_dataset('images', shape=(num_images, 256, 256, 3),\n",
    "                                         maxshape=(None, 256, 256, 3), dtype='float32', chunks=(1, 256, 256, 3))\n",
    "        \n",
    "        labels_dset = hdf.create_dataset('labels', shape=(num_images,), maxshape=(None,),\n",
    "                                         dtype=h5py.string_dtype())\n",
    "\n",
    "        for i, (image, label) in enumerate(image_label_pairs):\n",
    "            if image.dtype != np.float32:\n",
    "                image = image.astype('float32')\n",
    "            images_dset[i] = image\n",
    "            labels_dset[i] = label\n",
    "\n",
    "save_images_to_hdf5(data['ai'], ai_output_filename)\n",
    "save_images_to_hdf5(data['human'], human_output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been saved from the HDF5 files.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def save_images_from_hdf5(hdf5_filename, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with h5py.File(hdf5_filename, 'r') as hdf:\n",
    "        images = hdf['images'][:]\n",
    "        labels = hdf['labels'][:]\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        image = Image.fromarray((image * 255).astype('uint8'))\n",
    "\n",
    "        output_filename = os.path.join(output_folder, f'{label.decode()}_{i}.png')\n",
    "\n",
    "        image.save(output_filename, 'PNG')\n",
    "\n",
    "ai_hdf5_filename = 'ai_artwork_dataset.h5'\n",
    "human_hdf5_filename = 'human_artwork_dataset.h5'\n",
    "ai_output_folder = 'ai_images'\n",
    "human_output_folder = 'human_images'\n",
    "\n",
    "save_images_from_hdf5(ai_hdf5_filename, ai_output_folder)\n",
    "save_images_from_hdf5(human_hdf5_filename, human_output_folder)\n",
    "print(\"All images have been saved from the HDF5 files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extacted image file is too large for us to upload to Github, please check the folder google drive link if you would like to check images: https://drive.google.com/drive/folders/1UGPs65hHUtpPiEvUwpDFmNp5vpyx7EZz?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
